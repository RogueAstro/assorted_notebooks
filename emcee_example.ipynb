{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emcee usage example\n",
    "\n",
    "If you’re reading this right now then you’re probably interested in using emcee to fit a model to some noisy data. On this page, I’ll demonstrate how you might do this in the simplest non-trivial model that I could think of: fitting a line to data when you don’t believe the error bars on your data.\n",
    "\n",
    "This example is a reproduction of [this one](http://dan.iel.fm/emcee/current/user/line/), in iPython notebook fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as op\n",
    "import emcee\n",
    "import corner   # optional: necessary for corner plots\n",
    "                # see https://github.com/dfm/corner.py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative probabilistic model\n",
    "-------------\n",
    "\n",
    "When you approach a new problem, the first step is generally to write down the likelihood function (the probability of a dataset given the model parameters). This is equivalent to describing the generative procedure for the data. In this case, we’re going to consider a linear model where the quoted uncertainties are underestimated by a constant fractional amount. You can generate a synthetic dataset from this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the \"true\" parameters.\n",
    "m_true = -0.9594\n",
    "b_true = 4.294\n",
    "f_true = 0.534\n",
    "\n",
    "# The model function\n",
    "def model(x,m,b):\n",
    "    return m*x+b\n",
    "\n",
    "# Generate some synthetic data from the model.\n",
    "N = 50\n",
    "x = np.sort(10*np.random.rand(N))\n",
    "yerr = 0.1+0.5*np.random.rand(N)\n",
    "y_true = model(x,m_true,b_true) #m_true*x+b_true\n",
    "y = np.array([yk+np.abs(f_true*yk)*np.random.normal() for yk in y_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This synthetic dataset (with the underestimated error bars) will look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x,y,fmt='.',yerr=yerr,label='Data')\n",
    "plt.plot(x,y_true,linewidth=2,label='True curve')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true model is shown as the thick green line and the effect of the underestimated uncertainties is obvious when you look at this figure. The standard way to fit a line to these data (assuming independent Gaussian error bars) is linear least squares. Linear least squares is appealing because solving for the parameters—and their associated uncertainties—is simply a linear algebraic operation. Following the notation in [Hogg, Bovy & Lang (2010)](http://arxiv.org/abs/1008.4686), the linear least squares solution to these data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.vstack((np.ones_like(x),x)).T\n",
    "C = np.diag(yerr * yerr)\n",
    "cov = np.linalg.inv(np.dot(A.T, np.linalg.solve(C, A)))\n",
    "b_ls, m_ls = np.dot(cov, np.dot(A.T, np.linalg.solve(C, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[numpy.vstack](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.vstack.html): stack arrays in sequence vertically; we will have a Nx2 array whose first column is composed of ones, and the second column is the vector x. array.T: transposes the array. [numpy.diag](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.diag.html): creates a diagonal array. [numpy.linalg.solve](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linalg.solve.html): solves a linear matrix equation, or system of linear scalar equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataset generated above, the result is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('m = %.3f \\u00b1 %.3f' % (m_ls, cov[0,0]**0.5))\n",
    "print('b = %.3f \\u00b1 %.3f' % (b_ls, cov[1,1]**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotted below as a dashed red line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ls = model(x,m_ls,b_ls)\n",
    "plt.errorbar(x,y,fmt='.',yerr=yerr,label='Data')\n",
    "plt.plot(x,y_true,linewidth=2,label='True curve')\n",
    "plt.plot(x,y_ls,'--',linewidth=2,label='Least squares',color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn’t an unreasonable result but the uncertainties on the slope and intercept seem a little small (because of the small error bars on most of the data points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood estimation\n",
    "--------------\n",
    "The least squares solution found in the previous section is the maximum likelihood result for a model where the error bars are assumed correct, Gaussian and independent. We know, of course, that this isn’t the right model. Unfortunately, there isn’t a generalization of least squares that supports a model like the one that we know to be true. Instead, we need to write down the likelihood function and numerically optimize it. In mathematical notation, the correct likelihood function is:\n",
    "\n",
    "$\\ln{p \\left(y \\mid x,\\sigma,m,b,f \\right)} = -\\frac{1}{2} \\sum_n \\left[ \\frac{ \\left(y_n-m x_n-b \\right)^2}{s_n^2} + \\ln{ \\left(2 \\pi s_n^2 \\right)} \\right]$\n",
    "\n",
    "where\n",
    "\n",
    "$s_n^2 = \\sigma_n^2 + f^2(m x_n + b)^2$.\n",
    "\n",
    "This likelihood function is simply a Gaussian where the variance is underestimated by some fractional amount: f. In Python, you would code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnlike(theta, x, y, yerr):\n",
    "    m, b, lnf = theta\n",
    "    #model = m*x + b\n",
    "    mod = model(x,m,b)\n",
    "    inv_sigma2 = 1.0/(yerr*2 + mod**2*np.exp(2*lnf))\n",
    "    return -0.5*np.sum((y-mod)**2*inv_sigma2 + \\\n",
    "                       np.log(2.*np.pi/inv_sigma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, you’ll notice that I’m using the logarithm of f instead of f itself for reasons that will become clear in the next section. For now, it should at least be clear that this isn’t a bad idea because it will force f to be always positive. A good way of finding this numerical optimum of this likelihood function is to use the [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html) module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nll = lambda *args: -lnlike(*args)\n",
    "\n",
    "result = op.minimize(nll, [m_true, b_true, np.log(f_true)], \n",
    "                     args=(x, y, yerr))\n",
    "\n",
    "m_ml, b_ml, lnf_ml = result[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: [What is a lambda function in Python?](http://www.secnetix.de/olli/Python/lambda_functions.hawk)\n",
    "\n",
    "It’s worth noting that the optimize module minimizes functions whereas we would like to maximize the likelihood. This goal is equivalent to minimizing the negative likelihood (or in this case, the negative log likelihood). The maximum likelihood result is plotted as a solid black line — compared to the true model (green line) and linear least squares (dashed red line) — in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_ml = model(x,m_ml,b_ml)\n",
    "plt.errorbar(x,y,fmt='.',yerr=yerr,label='Data')\n",
    "plt.plot(x,y_true,linewidth=2,label='True curve')\n",
    "plt.plot(x,y_ls,'--',linewidth=2,label='Least squares',color='red')\n",
    "plt.plot(x,y_ml,linewidth=2,label='Max. likelihood',color='orange')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better! The values found by this optimization are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('m = %.3f' % (m_ml))\n",
    "print('b = %.3f' % (b_ml))\n",
    "print('f = %.3f' % (np.exp(lnf_ml)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem now: how do we estimate the uncertainties on m and b? What’s more, we probably don’t really care too much about the value of f but it seems worthwhile to propagate any uncertainties about its value to our final estimates of m and b. This is where MCMC comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginalization & uncertainty estimation\n",
    "--------------\n",
    "\n",
    "This isn’t the place to get into the details of why you might want to use MCMC in your research but it is worth commenting that a common reason is that you would like to marginalize over some “nuisance parameters” and find an estimate of the posterior probability function (the distribution of parameters that is consistent with your dataset) for others. MCMC lets you do both of these things in one fell swoop! You need to start by writing down the posterior probability function (up to a constant):\n",
    "\n",
    "$p(m,b,f \\mid x,y,\\sigma) \\propto p(m,b,f) p(y \\mid x,\\sigma,m,b,f)$.\n",
    "\n",
    "We have already, in the previous section, written down the likelihood function\n",
    "\n",
    "$p \\left(y \\mid x,\\sigma,m,b,f \\right)$\n",
    "\n",
    "so the missing component is the \"prior\" function\n",
    "\n",
    "$p(m,b,f)$.\n",
    "\n",
    "This function encodes any previous knowledge that we have about the parameters: results from other experiments, physically acceptable ranges, etc. It is necessary that you write down priors if you’re going to use MCMC because all that MCMC does is draw samples from a probability distribution and you want that to be a probability distribution for your parameters. This is important: **you cannot draw parameter samples from your likelihood function**. This is because a likelihood function is a probability distribution **over datasets** so, conditioned on model parameters, you can draw representative datasets (as demonstrated at the beginning of this exercise) but you cannot draw parameter samples.\n",
    "\n",
    "In this example, we’ll use uniform (so-called “uninformative”) priors on m, b and the logarithm of f. For example, we’ll use the following conservative prior on m:\n",
    "\n",
    "$p(m) = 1/5.5$, if $-5 \\lt m \\lt 1/2$\n",
    "\n",
    "$p(m) = 0$ otherwise.\n",
    "\n",
    "In code, the log-prior is (up to a constant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprior(theta):\n",
    "    m, b, lnf = theta\n",
    "    if -5.0 < m < 0.5 and 0.0 < b < 10.0 and -10.0 < lnf < 1.0:\n",
    "        return 0.0\n",
    "    return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, combining this with the definition of lnlike from above, the full log-probability function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprob(theta, x, y, yerr):\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(theta, x, y, yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all this setup, it’s easy to sample this distribution using <code>emcee</code>. We’ll start by initializing the walkers in a tiny Gaussian ball around the maximum likelihood result (I’ve found that this tends to be a pretty good initialization in most cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndim, nwalkers = 3, 100\n",
    "pos = [result[\"x\"] + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can set up the sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x, y, yerr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run the MCMC for 500 steps starting from the tiny ball defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler.run_mcmc(pos, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at what the sampler has done. The best way to see this is to look at the time series of the parameters in the chain. The sampler object now has an attribute called chain that is an array with the shape (100, 500, 3) giving the parameter values for each walker at each step in the chain. The figure below shows the positions of each walker as a function of the number of steps in the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, sharex=True, figsize=(6,6))\n",
    "axes[0].plot(sampler.chain[:,:,0].T)\n",
    "axes[0].axhline(y=m_true,linewidth=3,color='orange')\n",
    "axes[0].set_ylabel('m')\n",
    "axes[1].plot(sampler.chain[:,:,1].T)\n",
    "axes[1].axhline(y=b_true,linewidth=3,color='orange')\n",
    "axes[1].set_ylabel('b')\n",
    "axes[2].plot(np.exp(sampler.chain[:,:,2].T))\n",
    "axes[2].axhline(y=f_true,linewidth=3,color='orange')\n",
    "axes[2].set_ylabel('f')\n",
    "plt.xlabel('step number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true values of the parameters are indicated as orange lines on top of the samples. As mentioned above, the walkers start in small distributions around the maximum likelihood values and then they quickly wander and start exploring the full posterior distribution. In fact, after fewer than 50 steps, the samples seem pretty well “burnt-in”. That is a hard statement to make quantitatively but for now, we’ll just accept it and discard the initial 50 steps and flatten the chain so that we have a flat list of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = sampler.chain[:, 50:, :].reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "---------\n",
    "\n",
    "Now that we have this list of samples, let’s make one of the most useful plots you can make with your MCMC results: a corner plot. You’ll need the [corner.py](https://github.com/dfm/corner.py) module but once you have it, generating a corner plot is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(samples, labels=[\"$m$\", \"$b$\", \"$\\ln\\,f$\"],\n",
    "                      truths=[m_true, b_true, np.log(f_true)])\n",
    "fig.savefig(\"triangle.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corner plot shows all the one and two dimensional projections of the posterior probability distributions of your parameters. This is useful because it quickly demonstrates all of the covariances between parameters. Also, the way that you find the marginalized distribution for a parameter or set of parameters using the results of the MCMC chain is to project the samples into that plane and then make an N-dimensional histogram. That means that the corner plot shows the marginalized distribution for each parameter independently in the histograms along the diagonal and then the marginalized two dimensional distributions in the other panels.\n",
    "\n",
    "Another diagnostic plot is the projection of your results into the space of the observed data. To do this, you can choose a few (say 100 in this case) samples from the chain and plot them on top of the data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xl = np.array([0, 10])\n",
    "for m, b, lnf in samples[np.random.randint(len(samples), size=100)]:\n",
    "    plt.plot(xl, model(xl,m,b), color=\"k\", alpha=0.1)\n",
    "plt.plot(xl, model(xl,m_true,b_true), color=\"r\", lw=2, alpha=0.8)\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with one question: which numbers should go in the abstract? There are a few different options for this but my favorite is to quote the uncertainties based on the 16th, 50th, and 84th percentiles of the samples in the marginalized distributions. To compute these numbers for this example, you would run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples[:, 2] = np.exp(samples[:, 2])\n",
    "m_mcmc, b_mcmc, f_mcmc = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]),\n",
    "                             zip(*np.percentile(samples, [16, 50, 84],\n",
    "                                                axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving you the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('m = %.3f + (+%.3f, -%.3f)' % (m_mcmc[0],m_mcmc[1],m_mcmc[2]))\n",
    "print('b = %.3f + (+%.3f, -%.3f)' % (b_mcmc[0],b_mcmc[1],b_mcmc[2]))\n",
    "print('f = %.3f + (+%.3f, -%.3f)' % (f_mcmc[0],f_mcmc[1],f_mcmc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which isn’t half bad given the true values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('m_true = %.3f' % m_true)\n",
    "print('b_true = %.3f' % b_true)\n",
    "print('f_true = %.3f' % f_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
